:METADATA:
#+TITLE:        CompSci 413
#+AUTHOR:       JP Lozano
#+EMAIL:        jp@bassed.ca
#+DESCRIPTION:  Notes on Comp
#+KEYWORDS:     Philosophy,Notes
#+SUBTITLE:     Read at your own risk
#+DATE:         \today
:END:
:LATEX:
#+LATEX_CLASS: book
#+LATEX_CLASS_OPTIONS: [a4paper,titlepage,openany,oneside]
#+LATEX_HEADER:\usepackage{fullpage} \usepackage{fancyhdr} \usepackage{mathtools}
#+LATEX:\makeatletter
#+LATEX:\renewcommand{\@chapapp}{Section}
#+LATEX:\makeatother
#+LATEX:\maketitle
#+LATEX:\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
:END:
:OPTIONS:
#+OPTIONS: H:5 num:5
#+OPTIONS: num:nil toc:nil
:END:
* COMP413
** Intro
*** Introduction
    We gonna learn to (1) Formulate precise, accurate problem definitions. (2) Design  efficient, correct algorithms. (3) Analyze algorithms.
*** Formal Specifications of Problems
**** Pre-conditions and Post-conditions
     Each problem is specified by Pre-conditions: formal description of /acceptable/ inputs to the problem; Post-conditions: formal description of correct output and conditions it satisfies. 
     #+BEGIN_VERSE
     Example:
     Pre-conditions:
	* /A/: array of length /n/ >= 1
	* objects in /A/ are of some ordered type
     Post-conditions:
	* output integer /i/ such that
            0 <= /i/ < /n/
	    /A/[i] >= /A/[j] for every integer /j/ such that 0 <= /j/ <= /n/
	* Non-invasive algorithm 
     #+END_VERSE
**** Sorting an Array
     #+BEGIN_VERSE
     Pre-conditions:
	* /A/: array of length /n/ \geq 1
	* objects in /A/ are of some ordered type
     Post-condition:
	* elements of A have been reordered not replaced
	* /A/[i] \leq /A/[i+1] for 0 \leq i < /n/-1
     #+END_VERSE
**** Graph Search
     Take a graph and traverse all possible vertices
     #+BEGIN_VERSE
     Pre-conditions:
         * $G = (V,E)$, an undirected graph
	 * Source vertex $S \in V$
     Post-condition
         * representation of $\pi : V \to V \cup \{NIL\}$ such that the predecessor subgraph corresponding to $\pi$ is a spanning tree for the connected component of $G$ containing $S$
     #+END_VERSE
    
*** Stable Matching
    Originated by David Gale and Lloyd Shapley, the problem arose when they asked "Could one design a college admission process, or a job recruiting process, that was *self-enforcing*?"
**** Describe it
     Given a set of preferences among employers and applicants, can we assign applicants to employers so that for every employer /E/, and every applicant /A/ who is not scheduled to work for /E/, at least one of the following two things is the case?
     
     1. /E/ prefers every one of its accepted applicant to /A/; or
     2. /A/ prefers their current situation over working for employer /E/.
**** Formulating the Problem
     Each of the /n/ applicants applies to each of the /n/ companies, and each company wants to accept a /single/ applicant. Which is the same problem as each of /n/ men and /n/ women ending up married.

     Consider a set $M = {m_1,...,m_n}$ of /n/ men, and a set $W = {w_1,...,w_n}$ of /n/ women. Let $M \times W$ denote the set of all possible ordered pairs of the form $(m, w)$, where $m \in M$ and $w \in W$. A /matching/ $S$ is a /set/ of ordered pairs, each from $M \times W$, with the property that each member of $M$ and each member of $W$ appears in at most one pair in $S$. A /perfect matching/ $S'$ is a matching with the property that each member of $M$ and each member of $W$ appears in /exactly/ one pair in $S'$.
     
     Add /preference/. Each man $m \in M$ /ranks/ all the women; $m$ prefers $w$ to $w'$ if $m$ ranks $w$ higher than $w'$. This ordered ranking of $m$ is his /preference list/. No ties. Same for women.

     A matching $S$ is /stable/ if (i) i is perfect and (ii) there is no instability with respect to $S$. Rising two questions:

     1. Does there exist a matching for every set of preference list?
     2. Given a set of preference lists, can we efficiently construct a stable matching if there is one?

	
     #+BEGIN_VERSE
     Initially all $m \in M$ and all $w \in W$ are free

     While there is a man $m$ who is free and hasn't proposed to every woman
         Chose such a man $m$
	 Let $w$ be the highest-ranked woman in $m$'s preference list to whom $m$ has not yet proposed
	 If $w$ is free then
	     $(m, w)$ become engaged
	 Else $w$ is currently engaged to $m'$
	     If $w$ prefers $m'$ to $m$ then
	         $m$ remains free
	     Else $w$ prefers $m$ to $m'$
	         $(m,w)$ become engaged
		 $m'$ becomes free
	     Endif
         Endif
     Endwhile
     Return the set $S$ of engaged pairs 
     #+END_VERSE
     
     The G-S algorithm terminates after at most $n^2$ iterations of the While loop. 
*** Jan 16
**** Stable Matching
     Idea: given sets of $n$ men and $n$ women, find man/woman pairs so that everyone has a "best possible" match based on individual preference. What does the "best possible" mean?
***** Pre-conditions
      * set $M = \{m_1,...,m_n\}$ of men
      * set $W = \{w_1,...,w_n\}$ of women
      * for each man, ranking (ordered list) of all women in $W$
      * for each woman, ranking (ordered list) of all men in $M$


      Example:
      #+BEGIN_VERSE
      $n=2$
      $M=\{Albert,Bob\}$
      $W=\{Alice,Bess\}$

      _Preference List:_
      $Albert: \{Alice,Bess\}$
      $Bob:    \{Alice,Bess\}$
      $Alice:  \{Albert,Bob\}$
      $Bess:   \{Albert,Bob\}$

      _Perfect Matchings_
      $S_1 = \{ (Albert,Alice), (Bob,Bess) \}$
      $S_2 = \{ (Albert,Bess), (Bob,Alice) \}$
      
      _Stable Matching_
      $S_1 = \{ (Albert,Alice), (Bob,Bess) \}$
      
      In this stable matching $Albert$ is with his top choice. $Alice$ is also matched with her top choice. Should $Bob$ run to $Alice$ wanting to be paired up with her, she would deny the request as she is with her top choice.

      _Unstable Matching_
      $S_2 = \{ (Albert,Bess), (Bob,Alice) \}$

      In this case $Albert$ would want $Alice$, $Alice$ will agree to leaving $Bob$ because $Albert$ is highest ranked than $Bob$.
      #+END_VERSE
      
      A *Matching* is a set $S$ of ordered pairs $(M,W) \in M  \times W$ such that each $m \in M$ and $w \in W$ appears in /at most one/ pair of $S$.
      
      A *Perfect Matching* is the same exactly each $m$ and $w$ appear in exactly one pair. The number of stable matches is equal to $n!$.

      A *Stable Matching* is a perfect matching where no instability exists. *Instability* $(m,w) \in M \times W$ such that $m$ and $w$ prefer each other to their assigned partners in $S$.
      * $(m,w'),(m',w) \in S$, but $m$ prefers $w$ over $w'$ and $w$ prefers $m$ over $m'$
     

      #+BEGIN_VERSE
      Another Example:

      $n = 2, M = \{ m_1, m_2\} W = \{ w_1, w_2\}$
      _Preference List_
      
      $m_1: \{ w_1, w_2\}$
      $m_2: \{ w_2, w_1\}$
      $w_1: \{ m_2, m_1\}$
      $w_2: \{ m_1, m_2\}$

      _Perfect Matchings_
      $S_1 = \{ (m_1,w_1),(m_2,w_2) \}$
      $S_2 = \{ (m_1,w_2),(m_2,w_1) \}$

      _Stable Matchings_
      $S_1 = \{ (m_1,w_1),(m_2,w_2) \}$
      $S_2 = \{ (m_1,w_2),(m_2,w_1) \}$

      $S_1$ and $S_2$ are both stable, $S_1$ pairs both men to their preferred partners, $S_2$ pairs the women with their top choice.
      #+END_VERSE
      

      
*** Five Representative Problems
    Graph $G=(V,E)$ is /bipartite/ if its node set $V$ can be partitioned into sets $X$ and $Y$ in such a way that every edge has one end in $X$ and the other end in $Y$

    Given an arbitrary bipartite graph $G$, find a matching of maximum size. if $|X| = |Y| = n$, then there is a perfect matching if and only if the maximum matching has size $n$ 
** Basic of Algorithm Analysis
*** Computational Tractability
    An algorithm is /efficient/ if it has a polynomial running time. A /polynomial running time/ - There are absolute constants $c > 0$ and $d > 0$ so that on every input instance of size $N$, its running time is bounded by $cN^d$ primitive computational steps. If this running time holds, for some $c$ and $d$ the algorithm has a polynomial running time.
*** Asymptotic Order of Growth
**** \Omicron, \Omega, and \Theta
     *Asymptotic Upper Bounds.* Let $T(n)$ be a function--say, the worst case running time of a certain algorithm on an input size $n$. Given another function $f(n)$, we say that $T(n)$ is $O(f(n))$ (Read as "T(n) is order f(n)") if, for sufficiently large $n$, the function $T(n)$ is bounded above by a constant multiple of $f(n)$. 
     
     More precisely, $T(n)$ is $O(f(n))$ if there exists some constants $c > 0$ and $n_0 \geq 0$ so that for all $n \geq n_0$ we have $T(n) \leq c * f(n)$ In this case we say that $T$ is asymptotically upper bounded by $f$.
     
     *Asymptotic Lower Bounds* We want to express the notion that for arbitrary large input sizes $n$, the function $T(n)$ is /at least/ a constant multiple of some specific function $f(n)$. We say that $T(n)$ is $\Omega (f(n))$ if there exists constants $\epsilon > 0$ and $n_0 \geq 0$ so that for all $n \geq n_0$ we have $T(n) \geq \epsilon * f(n)$ We refer to $T$ as being asymptotically lower bounded by $f$

     *Asymptotically tight bounds*. If we can show a running time $T(n)$ is both $O(f(n)$ and also $\Omega (f(n))$ then in a natural sense we've found the "right" bound. We say that $T(n)$ is $\Theta (f(n))$
**** Properties of Asymptotic Growth Rates
     *Transitivity* If a function $f$ is asymptotically upper bounded by a function $g$, and if $g$ in turn is asymptotically upper bounded by a function $h$, then $f$ is asymptotically upper bounded by $h$. Similarly for lower bounds.
     
     * If $f = O(g)$ and $g = O(h)$, then $f = O(h)$
     * If $f = \Omega(g)$ and $g = \Omega(h)$, then $f = \Omega(h)$
     * If $f = \Theta(g)$ and $g = \Theta(h)$, then $f = \Theta(g)$

       
     *Sums of Functions*, adding them both

     * Suppose that $f$ and $g$ are two functions such that for some other function $h$, we have $f = O(h) and g = O(h)$ Then $f+g = O(h)$
     * Let $k$ be a fixed constant, and let $f_1,f_2,...,f_k$ and $h$ be functions such that $f_i=O(h)$ for all $i$. Then $f_1+f_2+...+f_k = O(h)$
**** Asymptotic Bounds for Some Common Functions
     */Polynomials/* A polynomial function can be written in the form $f(n)=a_0+a_1n+a_2n^2+...+a_dn^d$ for some integer constant $d>0$, where the final coefficient $a_d$ is nonzero. The value $d$ is the /degree/ of the polynomial. 
     * Let $f$ be a polynomial of degree $d$, in which the coefficient $a_d$ is positive. Then $f = O(n^d)$
       * From this it follows $f = \Omega(n^d)$ and hence $f = \Theta(n^d)$


     A *polynomial-time algorithm* is one whose running time $T(n)$ is $O(n^d)$ for some constant $d$, where $d$ is independent of input size.

     $O(n \log n)$ is also polynomial time. $\log n \leq n$ for all $n \geq 1$. Hence $n \log n \leq n^2$

     */Logarithms/* Recall $\log_bn$ is the number $x$ such that $b^x=n$

     * For every $b>1$ and every $x>0$, we have $\log_bn=O(n^x)$ 

       
     */Exponentials/* Exponential functions are of the form $f(n)=r^n$ for some constant base $r$

     * For every $r>1$ and every $d>0$ we have $n^d=O(r^n)$
       
     
       
     

     
     
     
 
     
*** Implementing the Stable Matching Algorithm Using Lists and Arrays
**** Arrays and Lists
     A list of $n$ elements can be kept in an array $A$ of size $n$, where $A[i]$ holds the $i^{th}$ element of the list.
     * We can answer a query of the form "What is the $i^{th}$ element on the list?" in $O(1)$ time, by direct access to the value $A[i]$
     * If we want t determine weather a particular element $e$ belongs to the list, we need to check the elements one by one in $O(n)$. Assuming no order is known.
     * If the array elements are sorted in some way, then we can determine weather element $e$ belongs to the list in $O(\log n)$

       
     It is hard to change a list implemented as an array. An alternate, preferable, way to mantain such a dynamic set of elements is via a /linked list/. In a /linked list/, the elements are sequenced together by having each element point to the next in the list. Thus, for each element $v$ on the list, we need to maintain a pointer to the next element; we set this pointer to ~null~ if $i$ is the last element. We also have a pointer ~First~ that points to the first element. We can traverse the list by starting at ~First~ and following until reaching ~null~. A dobly linked list can be modified as follows:
     * /Deletion/
     * /Insertion/

       
     Unlike Arrays one cannot find the $i^{th}$ element on a list in $O(1)$ time, rather one must traverse the list to the $i^{th}$ element, taking a total of $O(i)$ time.
*** A Survey of Common Running Times
**** Linear Time
     An algorithm that runs in $O(n)$, called linear time, has a very natural property: its running time is at most a constant factor times the size of the input. 
**** O(n \log n) Time
     It is the running time of any algorithm that splits its input into two equal-sized pieces, solves each piece recursively, and then combines the two solutions in linear time. 
**** Quadratic Time 
     Performing a search over all pairs of input items, spending constant time per pair. It also arises naturally from a pair of /nested loops/.
**** Cubic Time
     Algorithms running in $O(n^3)$ time. 
**** O(n^k) Time
     We obtain a running time of $O(n^k)$ for any constant $k$ when we search over all subsets of size $k$
**** Sub-Linear Time
     In general $O(\log n)$ arises as a time bound whenever we are dealing with an algorithm that does a constant amount of work in order to throw away a constant /function/ of the input. 
*** A more Complex Data Structure: Priority Queues
** Algorithm Analysis - According to Jacobson
*** Asymptotic Analysis
    Associated with any algorithm is a function $f : N \to N$ describing that algorithm's running time. most our analysis will be worst case, /upper bound/. 

    * The algorithm requires $f(n)$ steps in the *worst case* over all inputs of size $n$

      
    Unless otherwise stated, in one step we can perform:
    * Any arithmetic operation
    * Data movement
    * Control operation
    * Any similar operation
     

    Sometimes we do make other assumptions. For example, when discussing algorithms for arithmetic operations, we count /bit operations/. 

    What does it mean for a function to has input of size $n$? Depends on the problem, the size could be:
    * The dimensions of a matrix, or a pair of matrices
    * Numbers of vertices of a graph
    * The number of entries on a list 
    * The number of bits needed to encode input using some particular encoding scheme


    We will be capturing the *dominant term* of the function to describe the asymptotic growth rate. 
**** \Theta - Notation
     For a given function $g(n)$, define $\Theta(g(n))$ to be the set of functions. $$ \Theta(g(n)) = \{ f(n) | \exists c_1,c_2,n_0 \in R^+ s.t. 0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \forall n \geq n_0  \} $$
      
     * $f(n) \in O(g(n))$ means that $f(n)$ grows *no faster* than $cg(n)$
     * $f(n) \in \Omega(g(n))$ means that $f(n)$ grows *at least as fast* as $cg(n)$
       
       
     Example, show that $n \ln n \in o(n^2)$

     $$\lim_{n \to \infty} (n \ln n ) / n^2 = 0$$
     $$\lim_{n \to \infty} (\ln n ) / n = 0$$
     $$\lim_{n \to \infty} 1 / n = 0$$     
**** Helpful Facts
     1. Let $f(n)$ and $g(n)$ be functions
        * $f(n) \in \Theta(g(n)) \iff g(n) \Theta(f(n))$
        * $f(n) \in O(g(n)) \iff g(n) \in \Omega(f(n))$
        * $f(n) \in o(g(n)) \iff g(n) \in \omega(f(n))$
     2. Transitivity holds for all five.
	* $f(n) \in \Omega(g(n)), g(n) \in \Omega(h(n)) \to f(n) \in \Omega(h(n))$
     3. Let $f(n)$ and $g(n)$ be functions
	* $f(n) \in o(g(n)) \to f(n) \in O(g(n))$
	* $f(n) \in \omega(g(n)) \to f(n) \in \Omega(g(n))$
**** Polynomials
     A polynomial of degree $d$ is a constant function of the form
     $$p(n) = \sum_{n=0}^{d} a_in^i$$
     where $a_0,a_1,...,a_d$ are constants and $a_d$ \ne 0. If $p(n)$ is a polynomial of degree $d$, then $p(n) \in \Theta(n^d)$. This also holds for /real/ degrees.
**** Exponentials
     $f(n) = c^n$, this functions grow very quickly and it should be fixed. You should know rules for manipulating exponentials, like telling them they are ugly so they don't leave you. Let's see some examples
     * Let $c, e > 0$ be constants. Prove that $n^c = o((1 + e)^n)$
       $$ Prove this shit; \lim_{n \to \infty} (n^k / (1 + e) ) = 0 \forall k \in N$$
     $$ Base case: (k = 0): \lim_{n \to \infty} (n^0 / (1+e)^n ) $$
     $$  \lim_{n \to \infty} (1 / (1+e)^n ) = 0 $$
     $$ Induction hypothesis: Assume \lim_{n \to  \infty} (n^{k-1}/(1+e)^n)$$
     $$ Then  \lim_{n \to \infty} (n^k / (1+e)^n ) = \lim_{n \to \infty} ((kn^{k-1})/(ln(1+e)*(1+e)^n))$$
     $$ Then  \lim_{n \to \infty} (n^k / (1+e)^n ) = k / (ln(1+e) \lim_{n \to \infty} ((kn^{k-1})/((1+e)^n))$$
     $$ = 0 by induction hypothesis$$
**** Logarithms 
     Notations:
     * $\lg n = \log_2n$
     * $\ln n = \log_en$
     * $lg^kn = (\lg n)^k$
     * $\lg \lg n = \lg(\lg n)$
       
       
     $\lg n$  grows very slowly. $\lg^kn = o(n^e)$ for any constants $k,e>0$
**** Factorials
     $$ n! = \sqrt{2\pi n} (n/e)^n(1+\Theta(1/n))$$
     Examples: Let $f(n) = n^2\lg^3n,g(n)=(\sqrt{n})^4.5$ and find the faster one. To do this compute their limits, easy peasea. here we go:
     $$g(n)=(\sqrt{n})^{4.5}$$
     $$g(n)=(n^{0.5})^{4.5}$$
     $$g(n)=(n^{2.25})$$
     $$\lim_{n \to \infty} (f(n)/ g(n))$$
     $$\lim_{n \to \infty} (n^2 \lg^3n)/(n^{2.25}))$$
     $$\lim_{n \to \infty} (\lg^3n)/(n^{1/4})) = 0$$

     Another one:

     $$(\lg n)^{\lg n} or n$$
     $$n = 2^{\lg n}$$
     $$(\lg n)^{\lg n} = (2^{\lg \lg n})^{\lg n}$$
     $$(\lg n)^{\lg n} = (2^{(\lg \lg n)(\lg n)}$$
     $$(\lg n)^{\lg n} = (2^{(\lg n)})^{\lg \lg n}$$
     $$\lim_{n \to \infty} \frac{n}{(\lg n)^{\lg n}}$$
**** Bubble Sort
     #+BEGIN_SRC sude
     for i = 1 to length (A) do
     	for j = length (A) downto i+1 do
     		if A[j] < A[j-1] then
     			exchange A[j] and A[j-1]
     		endif
     	end for
     end for
     #+END_SRC
     Number of steps:
     $n^2$ is an upper bound of this algorithm $O(n^2)$ but we want a more precise definition of the growth rate. We are counting the number of steps it takes for the algorithm to finish. 
     #+BEGIN_VERSE
     We start with the first loop, which will go from ~i=1~ to ~n~
     When ~i=1~ how many steps will it take?
     In the inner loop, t is constant number of steps
     How many times will the inner loop iterate for ~i=1~?
     Total steps in the inner is $c(n-1)$
     When ~i=2~ it will be $c(n-2)$
     When ~i = (n-1)~  $c(n - (n-1))$
     When ~i=n~ $(n-n)$

     Total steps in the inner loop s the sum of this steps.
     $f(n) = c(n-1) + c(n-2) + ... + c(n - (n-1))$
     $$\sum_{i=1}^{n} c(n-i)$$
     $$(\sum_{i=1}^{n} cn ) - ( \sum_{i=1}^{n} ci )$$
     $$cn^2 - c( \sum_{i=1}^{n} i )$$
     $$cn^2 - c(\frac{n+1}{2})$$
     $$\frac{c}{2}n^2-\frac{c}{2}n$$
     $$\in\Theta(n^2)$$
     #+END_VERSE
**** Loops and Summations
     Loops generally result in /summations/. For example:
     #+BEGIN_VERSE
     	for i = 1 to n do
     		[something]
	end for
     #+END_VERSE
     Suppose [something] takes $ci^2$ steps:
     $$\sum_{i=1}^{n}ci^2$$
     Next up we have to do the algebra, so we can figure it out
***** Common sum
      1. Sum of squares and cubes
	 $$\sum_{k=1}^{n}k^2=\frac{n(n+1)(2n+1)}{6}\in\Theta(n^3)$$
	 $$\sum_{k=1}^{n}k^3=\frac{n^2(n+1)^2}{4}\in\Theta(n^4)$$
	 $$\sum_{k=1}{n}k^d\in\Theta(n^{d+1})$$
      2. Geometric series
	 $$\sum_{k=0}^{n}=1+\alpha+\alpha^2+...+\alpha^n=\frac{a^{n+1}-1}{\alpha-1}$$
      3. Harmonic series
	 $$H_n=\sum_{k=1}^{n}\frac{1}{k}=1+\frac{1}{2}+...+\frac{1}{n}$$
	 $$ln n + O(1)$$
**** Exmple of common sums
     What is the running time of:
     #+BEGIN_SRC c
     for(i=0;i<n;i++){
     	something
     }
     #+END_SRC
     something takes $\floor{\frac{n^2}{i}}$
     $$\sum_{i=0}^{n}\floor{\frac{n^2}{i}}$$
     $$n^2\sum_{i=1}^{n}\frac{1}{i}$$
     $$n^2(\ln+O(1))$$
     $$n^2\ln n + n^2O(1)$$
     $$\in O(n^2\ln n)$$
**** Example with no can do  
     $$\sum_{i=1}^{n}\floor{i \lg i}$$
     We need to find a lower bound and an upper bound. Split the sum in two: /This is gonna be nuts/
     $$\sum_{i=1}^{n}\floor{i \lg i} \ge \sum_{i=1}^{n}i \lg i - 1$$
     $$\sum_{i=1}^{n}i \lg i - \sum_{i=1}^{n}1$$
     $$\sum_{i=1}^{n}i \lg i - n$$
     $$(\sum_{i=1}^{\floor{n/2}}i \lg i + \sum_{\floor{n/2}+1}^{n}i \lg i) - n$$
     $$(\sum_{i=1}^{\floor{n/2}}i \lg i) \ge 0$$
     $$(n/2)(\sum_{\floor{n/2}+1}^{n}i \lg i) \ge (n/2) \lg (n/2)$$
     
